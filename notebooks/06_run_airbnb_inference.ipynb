{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FKnsO2DBeSkXSYA7aFzVAKPpuPjzFc2N",
      "authorship_tag": "ABX9TyNruXD2c5TuVuA/CHMQzXc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nantmoe-theingi/airbnb-nz-deception-sentiment/blob/main/notebooks/06_run_airbnb_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_DRIVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs(PROJECT_DRIVE_DIR, exist_ok=True)\n",
        "print(\"Drive project folder:\", PROJECT_DRIVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3xiESYZXCne",
        "outputId": "da22fd15-02dd-4e28-e142-332ccab186d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive project folder: /content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd {PROJECT_DRIVE_DIR}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2l9rl55XJpp",
        "outputId": "17478074-5794-478c-fee8-d9d61a365f4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new .py file and open it for editing\n",
        "file_name = \"run_airbnb_inference.py\"\n",
        "with open(file_name, \"w\") as f:\n",
        "    f.write(\"\")  # empty file created\n",
        "print(f\"{file_name} created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQfOLKPqSJr-",
        "outputId": "eee52ad6-45f4-436b-f544-c93a113db07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_airbnb_inference.py created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_airbnb_inference.py \\\n",
        "  --input_parquet data/airbnb_reviews_cleaned.parquet \\\n",
        "  --output_path outputs/airbnb_predictions.parquet \\\n",
        "  --deception_model_dir artifacts/distilbert_deception_sweep/best_model \\\n",
        "  --sentiment_model_dir artifacts/distilbert_sentiment_sweep/best_model"
      ],
      "metadata": {
        "id": "lMRjzLGC5wy4",
        "outputId": "936801b2-7d0b-400f-a2ea-a849fa6c59f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-28 22:21:36.786905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761690096.806776    2189 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761690096.812684    2189 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761690096.827735    2189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761690096.827773    2189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761690096.827777    2189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761690096.827780    2189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 22:21:36.832248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[DONE] Wrote 1,533,422 rows to outputs/airbnb_predictions.parquet in 3044.8s (504 rows/s).\n",
            "[VALIDATION] Output OK; predictions present and non-null.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_airbnb_inference.py \\\n",
        "  --input_parquet data/airbnb_reviews_cleaned.parquet \\\n",
        "  --output_path outputs/airbnb_predictions_partition/ \\\n",
        "  --partition_by year month \\\n",
        "  --deception_model_dir artifacts/distilbert_deception_sweep/best_model \\\n",
        "  --sentiment_model_dir artifacts/distilbert_sentiment_sweep/best_model"
      ],
      "metadata": {
        "id": "QKC8cbu-5mej",
        "outputId": "8c32e7bc-5897-4384-c67b-6e41ac973dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-28 23:18:29.928352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761693509.948310   16327 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761693509.954317   16327 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761693509.970534   16327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761693509.970559   16327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761693509.970563   16327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761693509.970566   16327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 23:18:29.975367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[DONE] Wrote 1,533,422 rows to outputs/airbnb_predictions_partition/ in 3049.7s (503 rows/s).\n",
            "[VALIDATION WARNING] Skipped or partial validation due to: 'pyarrow._dataset.FileSystemDataset' object has no attribute 'scan'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"outputs/airbnb_predictions.parquet\")\n",
        "print(df.shape)\n",
        "print(df.columns.tolist()[-12:])   # show prediction columns\n",
        "print(df[[\"sent_pred\", \"deception_pred\"]].value_counts().head())"
      ],
      "metadata": {
        "id": "F0fycImJ6c5r",
        "outputId": "359686c7-8c1a-4ec4-d7ec-19c4319368ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1533422, 27)\n",
            "['deception_logit_0', 'deception_logit_1', 'sent_logit_0', 'sent_logit_1', 'sent_logit_2', 'deception_prob_label_0', 'deception_prob_label_1', 'sent_prob_negative', 'sent_prob_neutral', 'sent_prob_positive', 'deception_pred', 'sent_pred']\n",
            "sent_pred  deception_pred\n",
            "2          0                 933980\n",
            "           1                 532944\n",
            "1          0                  38452\n",
            "0          0                  17971\n",
            "1          1                   5508\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "-IrbwIqOh2QK",
        "outputId": "e97b67fb-f3ec-4026-b469-6fd31681baa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['review_id', 'listing_id', 'date', 'text', 'price_num', 'property_type', 'room_type', 'host_is_superhost', 'neighbourhood', 'region_name', 'region_parent_name', 'region_parent_parent_name', 'region', 'year', 'month', 'deception_logit_0', 'deception_logit_1', 'sent_logit_0', 'sent_logit_1', 'sent_logit_2', 'deception_prob_label_0', 'deception_prob_label_1', 'sent_prob_negative', 'sent_prob_neutral', 'sent_prob_positive', 'deception_pred', 'sent_pred']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === Load your datasets ===\n",
        "reviews = pd.read_csv(\"data/airbnb_nz_reviews.csv\")\n",
        "listings = pd.read_csv(\"data/airbnb_nz_listings.csv\")\n",
        "preds = pd.read_parquet(\"outputs/airbnb_predictions.parquet\")\n",
        "\n",
        "RUNTIME_SECONDS = 3044.8  # from your log\n",
        "\n",
        "# --- 1) Canonicalise keys/dates exactly for the columns you specified ---\n",
        "def canon_id(s: pd.Series) -> pd.Series:\n",
        "    return s.astype(\"string\").str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "\n",
        "for df, cols in [\n",
        "    (reviews,  [\"id\", \"reviewer_id\"]),\n",
        "    (preds,    [\"review_id\", \"listing_id\", \"date\"]),\n",
        "    (listings, [\"id\"]),\n",
        "]:\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            if c == \"date\":\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "            else:\n",
        "                df[c] = canon_id(df[c])\n",
        "\n",
        "# --- 2) Use PREDICTIONS as the anchor ---\n",
        "# Join preds -> reviews ONLY to fetch reviewer_id for predicted rows\n",
        "preds_with_reviewer = preds.merge(\n",
        "    reviews[[\"id\", \"reviewer_id\"]],\n",
        "    how=\"left\",\n",
        "    left_on=\"review_id\",\n",
        "    right_on=\"id\"\n",
        ")\n",
        "\n",
        "# (optional) quick sanity checks\n",
        "predictions_generated = len(preds)  # count of rows that actually have predictions\n",
        "missing_reviewer_ids = preds_with_reviewer[\"reviewer_id\"].isna().sum()\n",
        "\n",
        "\n",
        "\n",
        "# --- 3) Compute metrics restricted to the prediction subset ---\n",
        "total_reviews = int(\"1533422\")  # full cleaned corpus (for coverage denominator)\n",
        "coverage = predictions_generated / total_reviews if total_reviews else np.nan\n",
        "\n",
        "# Unique listings seen in predictions\n",
        "unique_listings_pred = preds[\"listing_id\"].nunique()\n",
        "\n",
        "# Unique reviewers *whose reviews appear in preds*\n",
        "unique_reviewers_pred = preds_with_reviewer[\"reviewer_id\"].nunique()\n",
        "\n",
        "# Date span derived from preds['date'] (NOT from all reviews)\n",
        "if \"date\" in preds.columns and preds[\"date\"].notna().any():\n",
        "    date_lo, date_hi = preds[\"date\"].min(), preds[\"date\"].max()\n",
        "    date_span_txt = f\"{date_lo:%b~%Y}--{date_hi:%b~%Y}\"\n",
        "else:\n",
        "    date_span_txt = \"N/A\"\n",
        "\n",
        "throughput = predictions_generated / RUNTIME_SECONDS if RUNTIME_SECONDS else np.nan\n",
        "\n",
        "# --- 4) Build summary (values reflect prediction-anchored counts where applicable) ---\n",
        "def fmt_int(n): return f\"{int(n):,}\".replace(\",\", \"{,}\")\n",
        "def fmt_pct(x): return f\"{x*100:.2f}\\\\%\"\n",
        "\n",
        "summary_rows = [\n",
        "    (\"Total reviews processed\", \"Reviews in the cleaned Airbnb corpus\", fmt_int(total_reviews)),\n",
        "    (\"Predictions generated\",   \"Reviews with valid DistilBERT outputs\", fmt_int(predictions_generated)),\n",
        "    (\"Prediction coverage\",     \"Share of total reviews with predictions\", fmt_pct(coverage) if pd.notna(coverage) else \"N/A\"),\n",
        "    (\"Date span\",               \"Review period covered by predictions\", date_span_txt),\n",
        "    (\"Unique listings\",         \"Distinct listings represented in predictions\", fmt_int(unique_listings_pred)),\n",
        "    (\"Unique reviewers\",        \"Distinct reviewers represented in predictions\", fmt_int(unique_reviewers_pred)),\n",
        "    (\"Runtime (s)\",             \"Total inference execution time\", f\"{RUNTIME_SECONDS:,.1f}\".replace(\",\", \"{,}\")),\n",
        "    (\"Processing throughput\",   \"Reviews processed per second\", f\"{throughput:.0f}\" if pd.notna(throughput) else \"(insert value)\"),\n",
        "]\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows, columns=[\"Metric\", \"Description\", \"Value\"])\n",
        "\n",
        "# --- 5) Save CSV ---\n",
        "csv_path = \"figures/airbnb_inference_integrity_summary.csv\"\n",
        "df_summary.to_csv(csv_path, index=False)\n",
        "print(f\"Saved: {csv_path}\")\n",
        "print(f\"Missing reviewer_id after join: {missing_reviewer_ids:,}\")\n"
      ],
      "metadata": {
        "id": "Q0uElj2DgKRc",
        "outputId": "c3061486-6dbc-4b14-d639-4b75c73595e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: figures/airbnb_inference_integrity_summary.csv\n",
            "Missing reviewer_id after join: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "# --- Inputs ---\n",
        "# preds = pd.read_parquet(\"outputs/airbnb_predictions.parquet\")\n",
        "# Must contain:\n",
        "#   'deception_prob_label_1', 'sent_prob_negative', 'sent_prob_neutral',\n",
        "#   'sent_prob_positive', 'date'\n",
        "\n",
        "OUTDIR = \"figures\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# ============== Figure 1: Deception probability histogram ==============\n",
        "\n",
        "def plot_deception_histogram(df: pd.DataFrame,\n",
        "                             prob_col: str = \"deception_prob_label_1\",\n",
        "                             out_path: str = f\"{OUTDIR}/deception_probability_hist.png\"):\n",
        "    s = pd.to_numeric(df[prob_col], errors=\"coerce\").dropna()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # KDE-free histogram; fixed binning on [0,1]\n",
        "    plt.hist(s, bins=50, range=(0, 1))\n",
        "    plt.title(\"Distribution of Deception Probability\")\n",
        "    plt.xlabel(\"P(deceptive)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=300)\n",
        "    plt.close()\n",
        "    return out_path\n",
        "\n",
        "dec_hist_path = plot_deception_histogram(preds)\n",
        "\n",
        "# ============== Figure 2: Monthly stacked area of mean sentiment probs ==============\n",
        "\n",
        "def plot_monthly_sentiment_stack(df: pd.DataFrame,\n",
        "                                 date_col: str = \"date\",\n",
        "                                 neg_col: str = \"sent_prob_negative\",\n",
        "                                 neu_col: str = \"sent_prob_neutral\",\n",
        "                                 pos_col: str = \"sent_prob_positive\",\n",
        "                                 out_csv: str = f\"{OUTDIR}/monthly_sentiment_means.csv\",\n",
        "                                 out_png: str = f\"{OUTDIR}/sentiment_composition_stacked_area.png\"):\n",
        "    z = df.copy()\n",
        "    z[date_col] = pd.to_datetime(z[date_col], errors=\"coerce\")\n",
        "    z = z.dropna(subset=[date_col])\n",
        "\n",
        "    # Monthly mean of probabilities\n",
        "    monthly = (\n",
        "        z.set_index(date_col)[[neg_col, neu_col, pos_col]]\n",
        "         .resample(\"MS\").mean()\n",
        "    )\n",
        "\n",
        "    # (Optional) Re-normalize each month to sum to 1 (protect against rounding)\n",
        "    monthly = monthly.clip(lower=0)\n",
        "    row_sums = monthly.sum(axis=1)\n",
        "    monthly = monthly.div(row_sums.where(row_sums > 0, np.nan), axis=0)\n",
        "\n",
        "    # Save CSV for reproducibility\n",
        "    monthly_out = monthly.copy()\n",
        "    monthly_out.index.name = \"month\"\n",
        "    monthly_out.to_csv(out_csv)\n",
        "\n",
        "    # Stacked area plot (0–100%)\n",
        "    x = monthly.index\n",
        "    neg = monthly[neg_col].values\n",
        "    neu = monthly[neu_col].values\n",
        "    pos = monthly[pos_col].values\n",
        "\n",
        "    plt.figure(figsize=(9, 5.2))\n",
        "    plt.stackplot(x, neg, neu, pos, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Sentiment Probability Composition (Monthly Means)\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Share\")\n",
        "    plt.legend(loc=\"upper right\", frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    return out_csv, out_png\n",
        "\n",
        "sent_csv_path, sent_stack_path = plot_monthly_sentiment_stack(preds)\n",
        "\n",
        "print(\"Saved:\", dec_hist_path)\n",
        "print(\"Saved:\", sent_stack_path, \"and CSV:\", sent_csv_path)"
      ],
      "metadata": {
        "id": "dri2vyMgudM9",
        "outputId": "22e0570d-5170-4246-ae49-db9b26fe1e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: figures/deception_probability_hist.png\n",
            "Saved: figures/sentiment_composition_stacked_area.png and CSV: figures/monthly_sentiment_means.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "OUTDIR = \"figures\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# --- assume preds is your inference dataframe ---\n",
        "# preds['deception_pred'] : 0 = truthful, 1 = deceptive\n",
        "\n",
        "# 1️⃣ Count and normalize\n",
        "counts = preds[\"deception_pred\"].value_counts().sort_index()\n",
        "share = counts / counts.sum() * 100\n",
        "labels = [\"truthful\", \"deceptive\"]\n",
        "\n",
        "# 2️⃣ Create bar chart\n",
        "plt.figure(figsize=(6.5, 4.5))\n",
        "bars = plt.bar(labels, share, color=\"#2E6EBA\")\n",
        "\n",
        "# 3️⃣ Add text labels on top of each bar\n",
        "for bar, pct in zip(bars, share):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2,\n",
        "        bar.get_height() + 1,\n",
        "        f\"{pct:.1f}%\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=11,\n",
        "        fontweight=\"semibold\"\n",
        "    )\n",
        "\n",
        "# 4️⃣ Customize plot style\n",
        "plt.title(\"Distribution of Predicted Deception — Airbnb NZ (All Reviews)\", fontsize=12)\n",
        "plt.ylabel(\"Share of Reviews (%)\")\n",
        "plt.ylim(0, 105)\n",
        "plt.tight_layout()\n",
        "\n",
        "# 5️⃣ Save figure\n",
        "out_path = f\"{OUTDIR}/deception_predicted_bar.png\"\n",
        "plt.savefig(out_path, dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(f\"Saved bar chart to {out_path}\")\n"
      ],
      "metadata": {
        "id": "yRALotrXudJp",
        "outputId": "3c51c61d-7183-42f0-dec2-273b57f56b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved bar chart to figures/deception_predicted_bar.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QN1K-28ZudGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gAIRcPYudC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhIpKY1Zuc_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qr686ZC8uc8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(listings.columns.tolist())"
      ],
      "metadata": {
        "id": "eIr0y7N3iXPn",
        "outputId": "987c8194-72ca-4f36-c099-6f90b914f9c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'listing_url', 'scrape_id', 'last_searched', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'availability_eoy', 'number_of_reviews_ly', 'estimated_occupancy_l365d', 'estimated_revenue_l365d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'requires_license', 'license', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'region_id', 'region_name', 'region_parent_id', 'region_parent_name', 'region_parent_parent_id', 'region_parent_parent_name', 'reviews_per_month']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAkbhYP_ub9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiz_9quzub6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reTqK2nQub2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-RsQCgY2ubzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdi7lytSubue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDP2DhjSubiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_listings = merged_full[\"id\"].nunique()\n",
        "unique_reviewers = merged_full[\"listing_id\"].nunique()"
      ],
      "metadata": {
        "id": "x-VGysc1ffAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptzVh689fUUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# reviews_df: must contain at least ['id','listing_id','date','comments', ...]\n",
        "#             and ideally prediction columns if you wrote them back to this frame\n",
        "# listings_df: must contain at least ['id'] for unique listing count (optional but preferred)\n",
        "# runtime_seconds: total wall time your inference pass took (float or int). If unknown, set None.\n",
        "\n",
        "# Example:\n",
        "reviews_df = pd.read_parquet(\"outputs/airbnb_predictions.parquet\")\n",
        "listings_df = pd.read_csv(\"data/airbnb_nz_listings.csv\")\n",
        "runtime_seconds = None\n",
        "\n",
        "# --- Config (robust column picks) ---\n",
        "PRED_COLS = {\n",
        "    \"deception_pred\": [\"deception_pred\"],\n",
        "    \"sent_pred\": [\"sent_pred\"]\n",
        "}\n",
        "REVIEWER_ID_CANDIDATES = [\"reviewer_id\", \"reviewer\", \"user_id\", \"author_id\"]\n",
        "LISTING_ID_CANDIDATES = [\"listing_id\", \"listingId\", \"listing_id_str\"]\n",
        "\n",
        "def _pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _fmt_int_for_latex(n: int) -> str:\n",
        "    # Render 1,533,422 as 1{,}533{,}422 for LaTeX\n",
        "    return f\"{int(n):,}\".replace(\",\", \"{,}\")\n",
        "\n",
        "def _fmt_pct(p: float) -> str:\n",
        "    return f\"{p*100:.2f}\\\\%\"\n",
        "\n",
        "def _date_span_for_latex(series: pd.Series) -> str:\n",
        "    s = pd.to_datetime(series, errors=\"coerce\")\n",
        "    s = s.dropna()\n",
        "    if s.empty:\n",
        "        return \"N/A\"\n",
        "    lo, hi = s.min(), s.max()\n",
        "    # e.g., \"Oct~2022--Aug~2025\"\n",
        "    return f\"{lo.strftime('%b~%Y')}--{hi.strftime('%b~%Y')}\"\n",
        "\n",
        "def build_integrity_table(reviews_df: pd.DataFrame, listings_df: pd.DataFrame | None = None,\n",
        "                          runtime_seconds: float | None = None) -> tuple[pd.DataFrame, str]:\n",
        "    # Total reviews (cleaned corpus size)\n",
        "    total_reviews = len(reviews_df)\n",
        "\n",
        "    # Predictions generated (both deception & sentiment present and non-null)\n",
        "    has_deception = reviews_df[PRED_COLS[\"deception_pred\"][0]].notna() if PRED_COLS[\"deception_pred\"][0] in reviews_df else pd.Series(False, index=reviews_df.index)\n",
        "    has_sentiment = reviews_df[PRED_COLS[\"sent_pred\"][0]].notna() if PRED_COLS[\"sent_pred\"][0] in reviews_df else pd.Series(False, index=reviews_df.index)\n",
        "    has_preds = (has_deception & has_sentiment)\n",
        "    predictions_generated = int(has_preds.sum())\n",
        "    coverage = predictions_generated / total_reviews if total_reviews else np.nan\n",
        "\n",
        "    # Date span (from reviews.date)\n",
        "    date_span = _date_span_for_latex(reviews_df.get(\"date\", pd.Series(dtype=object)))\n",
        "\n",
        "    # Unique listings\n",
        "    listing_col = _pick_col(reviews_df, LISTING_ID_CANDIDATES)\n",
        "    unique_listings_from_reviews = reviews_df[listing_col].nunique() if listing_col else np.nan\n",
        "    if listings_df is not None and \"id\" in listings_df.columns:\n",
        "        unique_listings = listings_df[\"id\"].nunique()\n",
        "    else:\n",
        "        unique_listings = unique_listings_from_reviews\n",
        "\n",
        "    # Unique reviewers\n",
        "    rid_col = _pick_col(reviews_df, REVIEWER_ID_CANDIDATES)\n",
        "    unique_reviewers = reviews_df[rid_col].nunique() if rid_col else np.nan\n",
        "\n",
        "    # Throughput (rows/s), if runtime provided\n",
        "    if runtime_seconds and runtime_seconds > 0:\n",
        "        throughput = predictions_generated / float(runtime_seconds)\n",
        "    else:\n",
        "        throughput = np.nan\n",
        "\n",
        "    # Build a tidy DataFrame (Metric, Description, Value)\n",
        "    rows = [\n",
        "        (\"Total reviews processed\", \"Reviews in the cleaned Airbnb corpus\", _fmt_int_for_latex(total_reviews)),\n",
        "        (\"Predictions generated\", \"Reviews with valid DistilBERT outputs\", _fmt_int_for_latex(predictions_generated)),\n",
        "        (\"Prediction coverage\", \"Share of total reviews with predictions\", _fmt_pct(coverage) if np.isfinite(coverage) else \"N/A\"),\n",
        "        (\"Date span\", \"Review period after preprocessing\", date_span),\n",
        "        (\"Unique listings\", \"Distinct listings represented\", _fmt_int_for_latex(unique_listings) if pd.notna(unique_listings) else \"N/A\"),\n",
        "        (\"Unique reviewers\", \"Distinct reviewer identifiers\", _fmt_int_for_latex(unique_reviewers) if pd.notna(unique_reviewers) else \"(insert count)\"),\n",
        "        (\"Runtime (s)\", \"Total inference execution time\", f\"{runtime_seconds:.0f}\" if runtime_seconds else \"(insert value)\"),\n",
        "        (\"Processing throughput\", \"Reviews processed per second\", f\"{throughput:.2f}\" if pd.notna(throughput) else \"(insert value)\"),\n",
        "    ]\n",
        "    df = pd.DataFrame(rows, columns=[\"Metric\", \"Description\", \"Value\"])\n",
        "\n",
        "    # Emit LaTeX (booktabs) with your exact caption/label.\n",
        "    # Note: Escaping is already handled in values; descriptions are plain text.\n",
        "    latex_rows = \"\\n\".join([f\"{m} & {d} & {v} \\\\\\\\\" for m, d, v in rows])\n",
        "    latex_table = rf\"\"\"\n",
        "\\begin{{table}}[H]\n",
        "\\centering\n",
        "\\caption{{Prediction Coverage and Integrity Summary for Airbnb New Zealand Reviews.}}\n",
        "\\label{{tab:airbnb_inference_integrity}}\n",
        "\\begin{{tabular}}{{lcc}}\n",
        "\\toprule\n",
        "\\textbf{{Metric}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
        "\\midrule\n",
        "{latex_rows}\n",
        "\\bottomrule\n",
        "\\end{{tabular}}\n",
        "\\end{{table}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    return df, latex_table"
      ],
      "metadata": {
        "id": "lSMChjZjdykk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCIp89r5dygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnqwAX4zdyWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RQ1_temporal_analysis.py\n",
        "# End-to-end workflow for RQ1: temporal dynamics of deception & sentiment\n",
        "\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "INPUT_PATH = \"outputs/airbnb_predictions.parquet\"  # <- change if needed\n",
        "OUT_DIR = \"outputs/rq1_temporal\"\n",
        "ROLLING_WINDOW = 3  # months; set to 6 for 6-month smoothing\n",
        "DATE_COL = \"date\"\n",
        "\n",
        "# -----------------------------\n",
        "# Utils\n",
        "# -----------------------------\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _has(df, cols):\n",
        "    return all(c in df.columns for c in cols)\n",
        "\n",
        "def _maybe_to_datetime(df, col):\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def _percentify(series_float_0_1):\n",
        "    return series_float_0_1 * 100.0\n",
        "\n",
        "def safe_read_parquet(path):\n",
        "    try:\n",
        "        return pd.read_parquet(path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to read parquet at {path}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load & prepare\n",
        "# -----------------------------\n",
        "def load_predictions(path: str) -> pd.DataFrame:\n",
        "    df = safe_read_parquet(path)\n",
        "    df = _maybe_to_datetime(df, DATE_COL)\n",
        "    if df[DATE_COL].isna().all():\n",
        "        raise ValueError(f\"All {DATE_COL} values are NaT after parsing. Check your input.\")\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Monthly aggregation\n",
        "# -----------------------------\n",
        "def compute_monthly(df: pd.DataFrame, rolling_window: int = 3) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    # Ensure probabilities/labels exist; be robust to naming\n",
        "    # Deception prob\n",
        "    if \"deception_prob_deceptive\" not in df.columns:\n",
        "        # try common fallback names\n",
        "        candidates = [c for c in df.columns if c.startswith(\"deception_prob_\")]\n",
        "        if len(candidates) >= 2 and \"deception_prob_deceptive\" not in candidates:\n",
        "            # best effort: pick the column containing 'decept' as P(deceptive)\n",
        "            guess = [c for c in candidates if \"decept\" in c]\n",
        "            if guess:\n",
        "                df[\"deception_prob_deceptive\"] = df[guess[0]]\n",
        "            else:\n",
        "                raise KeyError(\"Could not find deception probability columns.\")\n",
        "        else:\n",
        "            raise KeyError(\"Missing 'deception_prob_deceptive' in predictions.\")\n",
        "\n",
        "    # Sentiment probabilities (neg/neu/pos). If not present, derive from sent_pred distribution\n",
        "    have_sent_probs = _has(df, [\"sent_prob_negative\", \"sent_prob_neutral\", \"sent_prob_positive\"])\n",
        "    if not have_sent_probs and \"sent_pred\" in df.columns:\n",
        "        # derive dummy probs from labels (hard 1.0 for the predicted class)\n",
        "        # assuming classes: 0=neg, 1=neu, 2=pos\n",
        "        for k in [\"sent_prob_negative\", \"sent_prob_neutral\", \"sent_prob_positive\"]:\n",
        "            if k not in df.columns:\n",
        "                df[k] = 0.0\n",
        "        df.loc[df[\"sent_pred\"] == 0, \"sent_prob_negative\"] = 1.0\n",
        "        df.loc[df[\"sent_pred\"] == 1, \"sent_prob_neutral\"]  = 1.0\n",
        "        df.loc[df[\"sent_pred\"] == 2, \"sent_prob_positive\"] = 1.0\n",
        "        have_sent_probs = True\n",
        "\n",
        "    # Sentiment positive share from labels (more stable for % positive)\n",
        "    def _pct_pos_from_pred(x: pd.Series):\n",
        "        # assume 2 = positive (your DistilBERT training)\n",
        "        return (x == 2).mean() if x.notna().any() else np.nan\n",
        "\n",
        "    # Year-month key\n",
        "    ym = pd.to_datetime(df[DATE_COL]).dt.to_period(\"M\")\n",
        "    df = df.copy()\n",
        "    df[\"year_month\"] = ym.astype(str)  # keep as string for csv friendliness\n",
        "    # Aggregate monthly\n",
        "    agg_dict = {\n",
        "        \"deception_prob_deceptive\": \"mean\",\n",
        "    }\n",
        "    if have_sent_probs:\n",
        "        agg_dict.update({\n",
        "            \"sent_prob_positive\": \"mean\",\n",
        "            \"sent_prob_negative\": \"mean\",\n",
        "            \"sent_prob_neutral\":  \"mean\",\n",
        "        })\n",
        "    if \"deception_pred\" in df.columns:\n",
        "        agg_dict[\"deception_pred\"] = \"mean\"\n",
        "    if \"sent_pred\" in df.columns:\n",
        "        agg_dict[\"sent_pred\"] = _pct_pos_from_pred\n",
        "\n",
        "    monthly = df.groupby(\"year_month\").agg(agg_dict).reset_index()\n",
        "\n",
        "    # Sort by time\n",
        "    monthly[\"year_month_dt\"] = pd.to_datetime(monthly[\"year_month\"])\n",
        "    monthly = monthly.sort_values(\"year_month_dt\")\n",
        "\n",
        "    # Rolling averages\n",
        "    roll_cols = [c for c in monthly.columns if c not in [\"year_month\", \"year_month_dt\"]]\n",
        "    monthly_roll = monthly.copy()\n",
        "    for c in roll_cols:\n",
        "        monthly_roll[c] = monthly_roll[c].rolling(rolling_window, min_periods=1).mean()\n",
        "\n",
        "    return monthly, monthly_roll\n",
        "\n",
        "# -----------------------------\n",
        "# Plots\n",
        "# -----------------------------\n",
        "def fig1_temporal_trends(monthly: pd.DataFrame, out_path: str):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    x = monthly[\"year_month_dt\"]\n",
        "\n",
        "    # Deception mean prob (as %)\n",
        "    if \"deception_prob_deceptive\" in monthly.columns:\n",
        "        y_dec = _percentify(monthly[\"deception_prob_deceptive\"])\n",
        "        plt.plot(x, y_dec, label=\"Mean P(deceptive) %\")\n",
        "\n",
        "    # Positive/negative sentiment (as %)\n",
        "    if \"sent_prob_positive\" in monthly.columns:\n",
        "        y_pos = _percentify(monthly[\"sent_prob_positive\"])\n",
        "        plt.plot(x, y_pos, label=\"Mean P(positive) %\")\n",
        "\n",
        "    if \"sent_prob_negative\" in monthly.columns:\n",
        "        y_neg = _percentify(monthly[\"sent_prob_negative\"])\n",
        "        plt.plot(x, y_neg, label=\"Mean P(negative) %\")\n",
        "\n",
        "    plt.title(\"Monthly Trends: Deception & Sentiment Probabilities\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Probability (%)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def fig2_dualaxis_deception_vs_positive(monthly: pd.DataFrame, out_path: str):\n",
        "    # Single figure (one plot), but show two series with a second y-axis\n",
        "    # (Matplotlib still counts this as one chart.)\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    x = monthly[\"year_month_dt\"]\n",
        "    y1 = monthly.get(\"deception_prob_deceptive\", pd.Series(index=monthly.index, dtype=float))\n",
        "    y2 = monthly.get(\"sent_prob_positive\", pd.Series(index=monthly.index, dtype=float))\n",
        "\n",
        "    ax1.plot(x, _percentify(y1), label=\"P(deceptive) %\")\n",
        "    ax1.set_xlabel(\"Month\")\n",
        "    ax1.set_ylabel(\"P(deceptive) (%)\")\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(x, _percentify(y2), label=\"P(positive) %\")\n",
        "    ax2.set_ylabel(\"P(positive) (%)\")\n",
        "\n",
        "    fig.suptitle(\"Dual-Axis Trend: Deception vs. Positive Sentiment\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "def fig3_heatmap_month_year(monthly: pd.DataFrame, value_col: str, out_path: str):\n",
        "    # Build Year x Month matrix for a chosen value (e.g., deception_prob_deceptive)\n",
        "    temp = monthly.copy()\n",
        "    temp[\"Year\"] = temp[\"year_month_dt\"].dt.year\n",
        "    temp[\"Month\"] = temp[\"year_month_dt\"].dt.month\n",
        "    pivot = temp.pivot(index=\"Year\", columns=\"Month\", values=value_col)\n",
        "\n",
        "    # Plot with imshow\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    im = plt.imshow(pivot, aspect=\"auto\", interpolation=\"nearest\")\n",
        "    plt.title(f\"Heatmap (Year x Month): {value_col}\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Year\")\n",
        "\n",
        "    # ticks\n",
        "    plt.xticks(ticks=np.arange(1, 13) - 1, labels=list(range(1, 13)))\n",
        "    plt.yticks(ticks=np.arange(len(pivot.index)), labels=pivot.index)\n",
        "\n",
        "    # colorbar\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Correlation & optional extras\n",
        "# -----------------------------\n",
        "def compute_correlations(monthly: pd.DataFrame) -> pd.DataFrame:\n",
        "    cols = []\n",
        "    if \"deception_prob_deceptive\" in monthly.columns:\n",
        "        cols.append(\"deception_prob_deceptive\")\n",
        "    if \"sent_prob_positive\" in monthly.columns:\n",
        "        cols.append(\"sent_prob_positive\")\n",
        "    if not cols:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    corr_pearson = monthly[cols].corr(method=\"pearson\")\n",
        "    corr_spearman = monthly[cols].corr(method=\"spearman\")\n",
        "\n",
        "    # Stack them with identifiers\n",
        "    out = []\n",
        "    for mname, cmat in [(\"pearson\", corr_pearson), (\"spearman\", corr_spearman)]:\n",
        "        c = cmat.reset_index().melt(id_vars=\"index\", var_name=\"var2\", value_name=\"corr\")\n",
        "        c = c.rename(columns={\"index\": \"var1\"})\n",
        "        c[\"method\"] = mname\n",
        "        out.append(c)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def optional_seasonal_decompose(monthly: pd.DataFrame, value_col: str, out_path: str):\n",
        "    try:\n",
        "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "    except Exception:\n",
        "        warnings.warn(\"statsmodels not installed; skipping seasonal decomposition.\")\n",
        "        return\n",
        "\n",
        "    s = monthly.set_index(\"year_month_dt\")[value_col].dropna()\n",
        "    # Ensure proper freq; infer monthly\n",
        "    s = s.asfreq(\"MS\")\n",
        "    try:\n",
        "        res = seasonal_decompose(s, model=\"additive\", period=12)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Decompose failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Plot each component in a single figure (still one chart)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    ax = plt.subplot(4, 1, 1); ax.plot(res.observed); ax.set_title(\"Observed\")\n",
        "    ax = plt.subplot(4, 1, 2); ax.plot(res.trend);    ax.set_title(\"Trend\")\n",
        "    ax = plt.subplot(4, 1, 3); ax.plot(res.seasonal); ax.set_title(\"Seasonal\")\n",
        "    ax = plt.subplot(4, 1, 4); ax.plot(res.resid);    ax.set_title(\"Residual\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def optional_change_points(monthly: pd.DataFrame, value_col: str, out_path: str, n_bkps: int = 3):\n",
        "    try:\n",
        "        import ruptures as rpt\n",
        "    except Exception:\n",
        "        warnings.warn(\"ruptures not installed; skipping change-point detection.\")\n",
        "        return\n",
        "\n",
        "    y = monthly[value_col].dropna().to_numpy().reshape(-1, 1)\n",
        "    if len(y) < (n_bkps + 2):\n",
        "        warnings.warn(\"Not enough points for change-point detection; skipping.\")\n",
        "        return\n",
        "\n",
        "    algo = rpt.Pelt(model=\"rbf\").fit(y)\n",
        "    bks = algo.predict(pen=10)  # heuristic; adjust if needed\n",
        "\n",
        "    # Plot signal with breakpoints\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(monthly[\"year_month_dt\"], y)\n",
        "    for b in bks[:-1]:\n",
        "        # b is index in the 1..N domain; align to x\n",
        "        if 0 <= b-1 < len(monthly):\n",
        "            xval = monthly[\"year_month_dt\"].iloc[b-1]\n",
        "            plt.axvline(xval)\n",
        "    plt.title(f\"Change-Point Detection: {value_col}\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(value_col)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main():\n",
        "    ensure_dir(OUT_DIR)\n",
        "    df = load_predictions(INPUT_PATH)\n",
        "\n",
        "    monthly, monthly_roll = compute_monthly(df, rolling_window=ROLLING_WINDOW)\n",
        "\n",
        "    # Save tables\n",
        "    monthly.to_csv(os.path.join(OUT_DIR, \"monthly_trends.csv\"), index=False)\n",
        "    monthly_roll.to_csv(os.path.join(OUT_DIR, \"monthly_trends_rolling.csv\"), index=False)\n",
        "\n",
        "    # Figures\n",
        "    fig1_temporal_trends(monthly, os.path.join(OUT_DIR, \"fig1_temporal_trends.png\"))\n",
        "    fig2_dualaxis_deception_vs_positive(monthly, os.path.join(OUT_DIR, \"fig2_dualaxis_deception_vs_positive.png\"))\n",
        "    # Heatmap on deception by default (you can switch value_col to \"sent_prob_positive\")\n",
        "    fig3_heatmap_month_year(monthly, \"deception_prob_deceptive\", os.path.join(OUT_DIR, \"fig3_heatmap_month_year.png\"))\n",
        "\n",
        "    # Correlations\n",
        "    corr = compute_correlations(monthly)\n",
        "    if not corr.empty:\n",
        "        corr.to_csv(os.path.join(OUT_DIR, \"correlations.csv\"), index=False)\n",
        "\n",
        "    # Optional extras (run only if libraries installed)\n",
        "    optional_seasonal_decompose(monthly, \"deception_prob_deceptive\", os.path.join(OUT_DIR, \"fig4_seasonal_decompose_deception.png\"))\n",
        "    optional_change_points(monthly, \"deception_prob_deceptive\", os.path.join(OUT_DIR, \"fig5_change_points.png\"))\n",
        "\n",
        "    print(f\"Done. Outputs saved in: {OUT_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8q7N7I94bab3",
        "outputId": "0290bccb-a55e-4c37-fb10-808db95b1be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Outputs saved in: outputs/rq1_temporal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1957896985.py:250: UserWarning: ruptures not installed; skipping change-point detection.\n",
            "  warnings.warn(\"ruptures not installed; skipping change-point detection.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6YT1VvEbaZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9l1SCEkRbaW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qB_6OtA4baT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Df-kxpVd6csn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_airbnb_inference.py \\\n",
        "  --input_parquet data/airbnb_reviews_cleaned.parquet \\\n",
        "  --output_parquet outputs/airbnb_predictions.parquet \\\n",
        "  --deception_model_dir \"/content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment/artifacts/distilbert_deception_sweep/best_model\" \\\n",
        "  --sentiment_model_dir \"/content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment/artifacts/distilbert_sentiment_sweep/best_model\""
      ],
      "metadata": {
        "id": "Ir-qGt6E2HAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_airbnb_inference.py \\\n",
        "  --input_parquet data/airbnb_reviews_cleaned.parquet \\\n",
        "  --output_parquet outputs/airbnb_predictions.parquet \\\n",
        "  --deception_model_dir \"/content/drive/MyDrive/Colab Notebooks/airbnb_nz_deception_sentiment/artifacts/deception_distilbert_hf\" \\\n",
        "  --sentiment_model_dir \"models/distilbert_tripadvisor\"\n"
      ],
      "metadata": {
        "id": "BOXP__nQVCXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d007a8-acdd-44d5-8413-15d1074a0012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-22 01:02:27.859102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761094947.923640   14048 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761094947.948451   14048 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761094947.998164   14048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761094947.998199   14048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761094947.998207   14048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761094947.998213   14048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 01:02:28.021758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[DONE] Wrote 1,533,422 rows to outputs/airbnb_predictions.parquet in 3109.7s (493 rows/s).\n",
            "[VALIDATION] Output shape OK; no null predictions.\n"
          ]
        }
      ]
    }
  ]
}